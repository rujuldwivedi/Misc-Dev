{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>\n",
    "<center> Indian Institute of Technology, Goa <center>\n",
    "<br>\n",
    "<center> Machine learning CS331 </center>\n",
    "<br>\n",
    "<center> Course Instructor: Dr Satyanath Bhat <center>\n",
    "<br>\n",
    "<center> Lab Assignment 2 </center> \n",
    "</H1>\n",
    "<H2>\n",
    "<center> GroupID: RuAsRaDe </center>\n",
    "<br>\n",
    "<center> Group Details </center>\n",
    "</H2>\n",
    "    \n",
    "| Name             | Roll No.|\n",
    "|------------------|---------|\n",
    "| Rujul Dwivedi | 2103319 |\n",
    "| Ashish Kumar Rathore | 2103304 |\n",
    "| Rahul Saini   | 2103318 |\n",
    "| Deepak Kumar  | 2103308 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for numerical computation\n",
    "import pandas as pd # for data manipulation\n",
    "from sklearn.datasets import fetch_california_housing , load_iris, load_digits # for loading the dataset\n",
    "from sklearn.preprocessing import Normalizer,OneHotEncoder # for normalizing the data and one hot encoding\n",
    "from sklearn.model_selection import train_test_split # for splitting the data\n",
    "from tqdm import trange # for progress bar\n",
    "import matplotlib.pyplot as plt # for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationLayer : # Layer 1\n",
    "    \"\"\"\n",
    "    Inputs : X ∈ R^(1xd) , W ∈ R^(dxK) # X is the input data, W is the weight matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W) : # This is the constructor of the class\n",
    "        self.X = X \n",
    "        self.W = W \n",
    "\n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \" An instance of Muliplication Layer.\"\n",
    "\n",
    "    def forward(self):  # This is the forward pass of the layer\n",
    "        self.Z = np.dot(self.X, self.W) # Z = XW\n",
    "\n",
    "    def backward(self): # This is the backward pass of the layer\n",
    "        self.dZ_dW = (self.X).T  # dZ/dW \n",
    "        self.dZ_daZ_prev = self.W  # dZ/dX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdditionLayer : # Layer 2\n",
    "    \"\"\"\n",
    "    Inputs : Z ∈ R^(1xK), B ∈ R^(1xK) # Z is the input data, B is the bias matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ): # This is the constructor of the class\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "    \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of Bias Addition Layer.\"\n",
    "    \n",
    "    def forward(self,): # This is the forward pass of the layer\n",
    "        self.Z = self.Z + self.B #Z = Z + B\n",
    "    \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] ) #dZ/dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLossLayer : # Layer 3\n",
    "    \"\"\"\n",
    "    Inputs : Y ∈ R^(1xK) , Y_hat ∈ R^(1xK) # Y is the true output, Y_hat is the predicted output\n",
    "    # aZ denotes output of previous activation layer \n",
    "    \"\"\"\n",
    "    def __init__(self, Y : np.ndarray , Y_hat : np.ndarray): # This is the constructor of the class\n",
    "        self.Y = Y \n",
    "        self.aZ = Y_hat \n",
    "    \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of Mean Squared Loss Layer\"\n",
    "    \n",
    "    def forward(self, ): # This is the forward pass of the layer\n",
    "        self.L = np.mean( ( self.aZ - self.Y)**2 ) #L = (1/n) * || Y_hat - Y||**2 \n",
    "        \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T   #dL/dY_hat = (2/n)*(Y_hat - Y).T      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxActivation : # Layer 4\n",
    "    \"\"\"\n",
    "    Input : a numpy array Z ∈ R^(1XK)  # Z is the input data\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): # This is the constructor of the class\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of Softmax Activation Layer\"\n",
    "        \n",
    "    def forward(self,): # This is the forward pass of the layer\n",
    "        self.aZ = self.softmax(self.Z) #aZ = softmax(Z).T\n",
    "    \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))  #daZ/dZ  = diag(aZ) - sZ*transpose(aZ)\n",
    "        # Shape = (K,K) where K = len( sZ )\n",
    "    \n",
    "    @staticmethod # We are making this method static as it does not depend on the object state and only on the input Z and why are we doing this? Because we can call this method without creating an instance of the class\n",
    "    def softmax(Z : np.ndarray): # This is the softmax function\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True ) #max_Z = max(Z)\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True) #softmax(Z) = exp(Z - max_Z)/sum(exp(Z - max_Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation : # Layer 5\n",
    "    \"\"\"\n",
    "    Input : a numpy array Z ∈ R^(Kx1) # Z is the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,Z ): # This is the constructor of the class\n",
    "        self.Z = Z \n",
    "    \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "    \n",
    "    def forward(self,): # This is the forward pass of the layer\n",
    "        self.aZ = self.sigmoid( self.Z )  # aZ = sigmoid( Z )\n",
    "    \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1) #aZ_i*(1-aZ_i)\n",
    "        self.daZ_dZ = np.diag(diag_entries) #daZ/dZ = diag(aZ_i*(1-aZ_i))\n",
    "    \n",
    "    @staticmethod # We are making this method static as it does not depend on the object state and only on the input Z and why are we doing this? Because we can call this method without creating an instance of the class\n",
    "    def sigmoid( Z : np.ndarray ) : # This is the sigmoid function\n",
    "        return  1./(1 + np.exp(-Z) ) #sigmoid(Z) = 1/(1 + exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer :  # Layer 6\n",
    "    \"\"\"\n",
    "    Inputs : Y ∈ R^(1xK) , Y_pred ∈ R^(1xK) # Y is the true output, Y_pred is the predicted output\n",
    "    \"\"\"    \n",
    "    def __init__(self, Y , Y_pred): # This is the constructor of the class\n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-40  \n",
    "        \n",
    "    \n",
    "    def __str__(self, ): # This is the string representation of the class\n",
    "        return \"An instance of Cross Entropy Loss Layer\"\n",
    "    \n",
    "    def forward(self, ): # This is the forward pass of the layer\n",
    "        self.L = - np.sum( self.Y * np.log(self.aZ+self.epsilon) ) #L = -1 * dot product of Y & log(Y_pred)\n",
    "        \n",
    "    def backward(self, ): # This is the backward pass of the layer\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T # dL/dY_pred ∈ R^(Kx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation : # Layer 7\n",
    "    \"\"\"\n",
    "    Input : Z ∈ R^(1xn) # Z is the input data\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): # This is the constructor of the class\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of Linear Activation.\"\n",
    "    \n",
    "    def forward(self, ): # This is the forward pass of the layer\n",
    "        self.aZ = self.Z  # aZ = Z\n",
    "    \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] ) #daZ/dZ = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanhActivation: # Layer 8\n",
    "    \"\"\"\n",
    "    Input : a numpy array Z ∈ R^(1xK)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Z): # This is the constructor of the class\n",
    "        self.Z = Z\n",
    "\n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of tanhActivation class.\"\n",
    "\n",
    "    def forward(self,): # This is the forward pass of the layer\n",
    "        self.aZ = np.tanh(self.Z) #aZ = tanh(Z)\n",
    "\n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2) #daZ/dZ = diag(1 - aZ**2) ∈ R^(KxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivation : # Layer 9\n",
    "    \"\"\"\n",
    "    Input : a numpy array Z ∈ R^(1xK)\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): # This is the constructor of the class\n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "    \n",
    "    def __str__(self,): # This is the string representation of the class\n",
    "        return \"An instance of ReLU activation\"\n",
    "    \n",
    "    def forward(self,): # This is the forward pass of the layer\n",
    "        self.aZ = np.maximum(self.Z,0) #aZ = max(Z,0)\n",
    "    \n",
    "    def backward(self,): # This is the backward pass of the layer\n",
    "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)]) #daZ/dZ = diag( 1 if aZ_i>0 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='california', \n",
    "             normalize_X=False, \n",
    "             normalize_y=False,\n",
    "             one_hot_encode_y = False, \n",
    "             test_size=0.2): # This function is used to load the dataset\n",
    "    if dataset_name == 'california' : \n",
    "        data = fetch_california_housing() # Load the california housing dataset\n",
    "    elif dataset_name == 'iris' : \n",
    "        data = load_iris() # Load the iris dataset\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = load_digits() # Load the mnist dataset\n",
    "        data['data'] = 1*(data['data']>=8) # Binarize the mnist dataset\n",
    "\n",
    "    X = data['data'] # X is the input data which we are clipping from the data's dictionary column name 'data'\n",
    "    y = data['target'].reshape(-1,1) # y is the output data which we are clipping from the data's dictionary column name 'target'\n",
    "    \n",
    "    if normalize_X == True : # normalising the input data to make the training process faster\n",
    "        normalizer = Normalizer() # Create an instance of the normalizer\n",
    "        X  = normalizer.fit_transform(X) # Normalize the input data\n",
    "    \n",
    "    if normalize_y == True : # normalising the output data to make the training process faster\n",
    "        normalizer = Normalizer() # Create an instance of the normalizer\n",
    "        y = normalizer.fit_transform(y) # Normalize the output data\n",
    "    \n",
    "# normalising makes the training process faster and more accurate because the range of the input and output data is reduced to a smaller range\n",
    "\n",
    "    if one_hot_encode_y == True: # one hot encode the output data to make it suitable for classification problems\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray() # One hot encode the output data\n",
    "        # y = np.eye(3)[y.reshape(-1)]\n",
    "\n",
    "# one hot encoding is used to convert the output data into a binary matrix because the output data is in the form of a vector and we need to convert it into a binary matrix to make it suitable for classification problems\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size) # Split the data into training and testing data according to the test size\n",
    "    return X_train, y_train, X_test, y_test # Return the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: # this is the most important class, it is the parent class of all the layers and it is used to create the neural network model given the input and output dimensions and the activation function\n",
    "                    # It is used to create the neural network model by stacking the layers on top of each other and connecting them to form a neural network model \n",
    "    \"\"\"\n",
    "    Input - activation : Activation Layer Name ,n_inp : dimension of input ,  n_out :  Number of output neurons \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inp, n_out, activation_name=\"linear\", seed=42): # This is the constructor of the class which initialises n_inp, n_out, activation_name and seed\n",
    "\n",
    "        np.random.seed(seed)  # for reproducability of code\n",
    "\n",
    "        self.n_inp = n_inp # dimension of input\n",
    "        self.n_out = n_out # Number of output neurons\n",
    "\n",
    "        # here X and Z denote input and output of the given layer respectively \n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random((1, n_inp))   # input\n",
    "        self.Z = np.random.random((1, n_out))  # output\n",
    "\n",
    "        # here W and B are initialized with some scaling to avoid over-flow for relu and tanh activation functions for regression problems and for sigmoid and softmax activation functions for classification problems\n",
    "\n",
    "        # Initialize W & B with some scaling to avoid over-flow\n",
    "        self.W = np.random.random((n_inp, n_out)) * \\\n",
    "            np.sqrt(2 / (n_inp + n_out)) # weight matrix; scaling is done by np.sqrt(2 / (n_inp + n_out)) which is called as He initialization\n",
    "        self.B = np.random.random((1, n_out))*np.sqrt(2 / (1 + n_out)) # bias matrix; scaling is done by np.sqrt(2 / (1 + n_out)) which is called as He initialization\n",
    "        # He initialization is used to initialize the weights of the neural network by scaling the weights according to the number of input and output neurons using np.sqrt(2 / (n_inp + n_out))\n",
    "        # define multiplication layer, bias addition layer , and activation layer\n",
    "\n",
    "        self.multiply_layer = MultiplicationLayer(self.X, self.W) # create an instance of the MultiplicationLayer class\n",
    "        self.bias_add_layer = BiasAdditionLayer(self.B, self.B) # create an instance of the BiasAdditionLayer class\n",
    "\n",
    "        if activation_name == 'linear':\n",
    "            self.activation_layer = LinearActivation(self.Z) # create an instance of the LinearActivation class\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation_layer = SigmoidActivation(self.Z) # create an instance of the SigmoidActivation class\n",
    "        elif activation_name == 'softmax':\n",
    "            self.activation_layer = SoftMaxActivation(self.Z) # create an instance of the SoftMaxActivation class\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation_layer = tanhActivation(self.Z) # create an instance of the tanhActivation class\n",
    "        elif activation_name == 'relu':\n",
    "            self.activation_layer = ReLUActivation(self.Z) #create an instance of the ReLUActivation class\n",
    "\n",
    "        \"\"\"\n",
    "        The forward pass works as follows:\n",
    "        The input X is passed to the multiplication layer which multiplies the input X with the weight matrix\n",
    "        W to get the output Z and then the output Z is passed to the bias addition layer which adds the bias matrix B to the output Z\n",
    "        to get the next output Z and then this output Z is passed to the activation layer which applies the activation function to the output Z\n",
    "        to get the next output Z and then this output Z is the final output of the given layer \n",
    "        \"\"\"\n",
    "    def forward(self,): # forward pass of the layer\n",
    "        self.multiply_layer.X = self.X # input to the multiplication layer\n",
    "        self.multiply_layer.forward() # forward pass of the multiplication layer\n",
    "\n",
    "        self.bias_add_layer.Z = self.multiply_layer.Z # input to the bias addition layer\n",
    "        self.bias_add_layer.forward() # forward pass of the bias addition layer\n",
    "\n",
    "        self.activation_layer.Z = self.bias_add_layer.Z # input to the activation layer\n",
    "        self.activation_layer.forward() # forward pass of the activation layer\n",
    "\n",
    "        self.Z = self.activation_layer.aZ  # output of given layer\n",
    "\n",
    "        \"\"\"\n",
    "        The backward pass works as follows:\n",
    "        The output Z is passed to the activation layer which applies the activation function to the output Z\n",
    "        to get the derivative of the output Z with respect to the input and then this derivative is passed to the multiplication layer\n",
    "        which gets the derivative of the previous output Z with respect to the weight matrix W and then this derivative is passed to the bias addition layer\n",
    "        which gets the derivative of the previous output Z with respect to the bias matrix B and then this derivative is the final derivative of the output Z\n",
    "        \"\"\"\n",
    "\n",
    "    def backward(self,): # backward pass of the layer\n",
    "        self.activation_layer.backward() # backward pass of the activation layer\n",
    "        self.bias_add_layer.backward() # backward pass of the bias addition layer\n",
    "        self.multiply_layer.backward() # backward pass of the multiplication layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(Layer): # This class is used to create the neural network model by stacking the layers on top of each other and connecting them to form a neural network model\n",
    "    \"\"\"\n",
    "    Input  - layers : list of layer objects , loss_name : Name of loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    # [ \"mean_squared\", \"cross_entropy\"]\n",
    "    def __init__(self, layers, loss_name=\"mean_squared\", learning_rate=0.01, seed=42): # This is the constructor of the class which initialises layers, loss_name, learning_rate and seed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers # list of layer objects\n",
    "        self.n_layers = len(layers)  # number of layers\n",
    "        self.learning_rate = learning_rate # learning rate\n",
    "\n",
    "        self.inp_shape = self.layers[0].X.shape # input shape\n",
    "        self.out_shape = self.layers[-1].Z.shape # output shape\n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random(self.inp_shape)   # input of neural network\n",
    "        self.Y = np.random.random(self.out_shape)  # output of neural network\n",
    "\n",
    "        # define loss layer\n",
    "        if loss_name == \"mean_squared\":\n",
    "            self.loss_layer = MeanSquaredLossLayer(self.Y, self.Y) # create an instance of the MeanSquaredLossLayer class\n",
    "        if loss_name == \"cross_entropy\":\n",
    "            self.loss_layer = CrossEntropyLossLayer(self.Y, self.Y) # create an instance of the CrossEntropyLossLayer class\n",
    "\n",
    "    \"\"\"\n",
    "    The forward pass works as follows:\n",
    "    The input X is passed to the first layer which applies the forward pass to the input X to get the next output Z\n",
    "    and then this output Z is passed to the next layer which applies the forward pass to this output Z to get the next output Z\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self,): # forward pass of the neural network\n",
    "        self.layers[0].X = self.X # input to the first layer\n",
    "        self.loss_layer.Y = self.Y # true output\n",
    "\n",
    "        self.layers[0].forward() # forward pass of the first layer\n",
    "        for i in range(1, self.n_layers): # forward pass of the remaining layers\n",
    "            self.layers[i].X = self.layers[i-1].Z # input to the next layer\n",
    "            self.layers[i].forward() # forward pass of the next layer\n",
    "\n",
    "        self.loss_layer.aZ = self.layers[-1].Z # predicted output\n",
    "        self.loss_layer.forward() # forward pass of the loss layer\n",
    "\n",
    "    \"\"\"\n",
    "    The backward pass works as follows:\n",
    "    The predicted output Z is passed to the loss layer which applies the backward pass to the predicted output Z to get the derivative\n",
    "    of the predicted output Z with respect to the true output and then this derivative is passed to the last layer which applies the\n",
    "    backward pass to the derivative of the predicted output Z\n",
    "    \"\"\"\n",
    "\n",
    "    def backward(self,): # backward pass of the neural network\n",
    "\n",
    "        self.loss_layer.Z = self.Y # predicted output\n",
    "        self.loss_layer.backward() # backward pass of the loss layer\n",
    "        self.grad_nn = self.loss_layer.dL_daZ # derivative of the predicted output Z with respect to the true output\n",
    "        for i in range(self.n_layers-1, -1, -1): # backward pass of the layers\n",
    "            self.layers[i].backward() # backward pass of the layer\n",
    "\n",
    "            dL_dZ = np.dot(\n",
    "                self.layers[i].activation_layer.daZ_dZ, self.grad_nn) # dL/dZ\n",
    "            dL_dW = np.dot(self.layers[i].multiply_layer.dZ_dW, dL_dZ.T) # dL/dW\n",
    "            dL_dB = np.dot(self.layers[i].bias_add_layer.dZ_dB, dL_dZ).T # dL/dB\n",
    "\n",
    "            # Update W & B\n",
    "            self.layers[i].W -= self.learning_rate*dL_dW # update weight matrix\n",
    "            self.layers[i].B -= self.learning_rate*dL_dB # update bias matrix\n",
    "\n",
    "            # Update outer_grad\n",
    "            self.grad_nn = np.dot(\n",
    "                self.layers[i].multiply_layer.dZ_daZ_prev, dL_dZ) # dL/dZ_prev\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB # delete the variables to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLayers(inp_shape, layers_sizes, layers_activations): # This function is used to add the layers to the neural network model given the input and output dimensions and the activation function\n",
    "    # This function works as follows: It creates an instance of the Layer class for each layer and appends the layer to the list of layers to create the neural network model \n",
    "    layers = [] # list of layers\n",
    "    n_layers = len(layers_sizes) # number of layers\n",
    "    layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0]) # create an instance of the Layer class\n",
    "    layers.append(layer_0) # append the layer to the list of layers\n",
    "    inp_shape_next = layers_sizes[0] # input shape of the next layer\n",
    "    for i in range(1, n_layers): # create the remaining layers\n",
    "        layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i]) # create an instance of the Layer class \n",
    "        layers.append(layer_i) # append the layer to the list of layers\n",
    "        inp_shape_next = layers_sizes[i] # input shape of the next layer\n",
    "\n",
    "    out_shape = inp_shape_next # output shape\n",
    "    return inp_shape, out_shape, layers # return the input shape, output shape and the list of layers\n",
    "# Note that this function and the class Neural Network do the same thing but this function is used to create the layers of the neural network model and the class Neural Network is used to create the neural network model\n",
    "# That is, first we create the layers of the neural network model one by one using this function and then we create the neural network model using the class Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent is used to train the model and it is a type of gradient descent in which instead of using the entire\n",
    "# dataset to compute the gradient of the cost function in each iteration, it uses only a randomly chosen sample subset of the data for each iteration.\n",
    "def SGD_NeuralNetwork(X_train,\n",
    "                      y_train,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      nn,\n",
    "                      inp_shape=1, \n",
    "                      out_shape=1,\n",
    "                      n_iterations=1000,\n",
    "                      task=\"regression\"\n",
    "                      ): # This function is used to train the neural network model using stochastic gradient descent\n",
    "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100) # progress bar\n",
    "\n",
    "    for iteration, _ in enumerate(iterations): # train the model for each iteration\n",
    "        randomIndx = np.random.randint(len(X_train)) # randomly choose a sample subset of the data\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape) # input data\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape) # output data\n",
    "\n",
    "        nn.X = X_sample # initialize the input data to the sample subset of the data\n",
    "        nn.Y = Y_sample # initialize the output data to the sample subset of the data\n",
    "\n",
    "        nn.forward()  # Forward Pass\n",
    "        nn.backward()  # Backward Pass\n",
    "\n",
    "    # Now we'll run only forward pass for train and test data and check accuracy/error because we have already updated the weights and biases in the backward pass\n",
    "\n",
    "    if task == \"regression\": # check the error for regression problems\n",
    "        \n",
    "        nn.X = X_train \n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        train_error = nn.loss_layer.L # error for training data\n",
    "        \n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        test_error = nn.loss_layer.L # error for testing data\n",
    "\n",
    "        if isinstance(nn.loss_layer, MeanSquaredLossLayer): # check the error for mean squared loss layer\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\" % train_error)\n",
    "            print(\"Mean Squared Loss Error (Test Data)  : %0.5f\" % test_error)\n",
    "\n",
    "    if task == \"classification\": # check the accuracy for classification problems\n",
    "        \n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_train, axis=1) \n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1) \n",
    "        acc = 1*(y_true == y_pred) # accuracy for training data\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc))) # {0}/{1} = {2} is used to format the output\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred) # accuracy for testing data\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc))) # {0}/{1} = {2} is used to format the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('california', normalize_X=True, normalize_y=False, test_size=0.2) # load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:01<00:00, 5131.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 1.43392\n",
      "Mean Squared Loss Error (Test Data)  : 1.42327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1] # input shape\n",
    "layers_sizes = [1] # number of neurons in each layer\n",
    "layers_activations = ['linear'] # activation function for each layer\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'mean_squared' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1) # create the neural network model\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"regression\") # train the neural network model using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...:   0%|                                                        | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 3184.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 1.33825\n",
      "Mean Squared Loss Error (Test Data)  : 1.32905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1] # input shape\n",
    "layers_sizes = [13,1] # number of neurons in each layer\n",
    "layers_activations = ['sigmoid','linear'] # activation function for each layer\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'mean_squared' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01) # create the neural network model\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\") # train the neural network model using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 3557.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 1.33448\n",
      "Mean Squared Loss Error (Test Data)  : 1.32596\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1] # input shape\n",
    "layers_sizes = [13,13,1] # number of neurons in each layer\n",
    "layers_activations = ['sigmoid','sigmoid','linear'] # activation function for each layer\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'mean_squared' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.001) # create the neural network model\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\") # train the neural network model using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True, test_size=0.3) # load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:02<00:00, 4140.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1188/1257 = 94.5107398568019 %\n",
      "Classification Accuracy (Testing Data ): 482/540 = 89.25925925925925 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1] # input shape\n",
    "layers_sizes = [89,10] # number of neurons in each layer\n",
    "layers_activations = ['tanh','sigmoid'] # activation function for each layer\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'mean_squared' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1) # create the neural network model\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\") # train the neural network model using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:02<00:00, 3410.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1203/1257 = 95.70405727923628 %\n",
      "Classification Accuracy (Testing Data ): 496/540 = 91.85185185185185 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1] # input shape\n",
    "layers_sizes = [89,10] # number of neurons in each layer\n",
    "layers_activations = ['tanh','softmax'] # activation function for each layer\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'cross_entropy' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01) # create the neural network model\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\") # train the neural network model using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we are given single channel input and initial filter to be a 3x3 matrix:\n",
    "\n",
    "def convolutional_layer(zero_pad_input, l_filter):\n",
    "    inp = zero_pad_input  # input matrix\n",
    "    l = len(inp)  # length of input matrix\n",
    "    m = len(l_filter)  # length of filter\n",
    "    c = len(zero_pad_input)  # size of zero-padded matrix\n",
    "    s = (c - m) + 1  # to be used for loop for filtering\n",
    "    out = np.zeros((l, l))  # output after convolution\n",
    "\n",
    "    # filtering\n",
    "    for i in range(s): \n",
    "        for j in range(s):\n",
    "            temp = np.zeros((m, m)) # temporary matrix to store the filtered matrix\n",
    "            row, col = np.indices((m, m)) # indices of the filter matrix\n",
    "            temp = np.multiply(zero_pad_input[row+i, col+j], l_filter) # element-wise multiplication of input matrix and filter matrix\n",
    "\n",
    "            out[i][j] = np.sum(temp) # sum of the elements of the temporary matrix\n",
    "    return out # output after convolution\n",
    "\n",
    "# filtering is done to extract features from the input matrix and it is done by element-wise multiplication of the input matrix and the filter matrix and then summing the elements of the temporary matrix to get the output after convolution\n",
    "# filtering helps in reading features from the input matrix and it is done by sliding the filter matrix over the input matrix till the end of the input matrix\n",
    "def Forward_pass(inp, l_filter): # Forward pass implementation\n",
    "    l = len(inp) # length of input matrix\n",
    "    zero_pad_input = np.zeros((l+2, l+2)) # zero-padded input matrix\n",
    "    zero_pad_input[1:l+1, 1:l+1] = inp # here we are setting the input matrix in the center of the zero-padded input matrix\n",
    "\n",
    "    f_out = convolutional_layer(zero_pad_input, l_filter) # output after convolution\n",
    "    return f_out \n",
    "\n",
    "def rotateMatrix(mat): # Rotate the matrix by 180 degree\n",
    "    N = len(mat) # length of the matrix\n",
    "    rot_mat = np.zeros((N, N)) # rotated matrix\n",
    "    k = N - 1 # index of the last element\n",
    "    t1 = 0 # index of the first element\n",
    "    while (k >= 0 and t1 < 3):\n",
    "        j = N - 1 \n",
    "        t2 = 0\n",
    "        while (j >= 0 and t2 < N):\n",
    "            rot_mat[t1][t2] = mat[k][j]\n",
    "            j = j - 1\n",
    "            t2 = t2 + 1\n",
    "        k = k - 1\n",
    "        t1 = t1 + 1\n",
    "\n",
    "    return rot_mat\n",
    "\n",
    "# We are rotating the filter matrix by 180 degree to apply convolution to the input matrix to get the gradient of the loss w.r.t input matrix and the gradient of the loss w.r.t filter matrix in the backward pass \n",
    "\n",
    "def Backward_pass(inp, output, l_filter): # Backward pass implementation\n",
    "    l = len(inp) # length of input matrix\n",
    "    zero_pad_input = np.zeros((l+2, l+2)) # zero-padded input matrix\n",
    "    zero_pad_input[1:l+1, 1:l+1] = inp # here we are setting the input matrix in the center of the zero-padded input matrix\n",
    "\n",
    "    grad_filter = convolutional_layer(zero_pad_input, output) # gradient of loss w.r.t filter matrix\n",
    "    # we can use gradient of filter coefficient matrix to update the filter matrix:\n",
    "    # filter = filter - learning_rate * gradient of filter coefficient matrix\n",
    "\n",
    "    # for gradient of loss w.r.t input, we need to rotate the filter by 180° and apply convolution.\n",
    "\n",
    "    rotated_filter = rotateMatrix(l_filter) # rotate the filter matrix by 180 degree\n",
    "    zero_pad_output = np.zeros((l+2, l+2)) # zero-padded output matrix\n",
    "    zero_pad_output[1:l+1, 1:l+1] = output # here we are setting the output matrix in the center of the zero-padded output matrix\n",
    "    grad_X = convolutional_layer(zero_pad_output, rotated_filter) # gradient of loss w.r.t input matrix\n",
    "\n",
    "    return grad_filter, grad_X\n",
    "\n",
    "def flatten(inp_mat): # Flatten the matrix\n",
    "    flatten_vector = [] # flattened vector\n",
    "\n",
    "    for i in range(len(inp_mat)):  # number of rows\n",
    "        for j in range(len(inp_mat[0])):  # number of columns\n",
    "            flatten_vector.append(inp_mat[i][j]) # append the elements of the matrix to the flattened vector\n",
    "\n",
    "    flatten_vector = np.array(flatten_vector) # convert the flattened vector to a numpy array\n",
    "    return flatten_vector \n",
    "\n",
    "# flattening is done to convert the matrix into a vector and it is done by appending the elements of the matrix to the flattened vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer: # Implementation of Convolutional Layer consist of Convolution  followed by flattening  and Activation operation\n",
    "    def __init__(self,\n",
    "                 # inp_shape = (input_channels, input_height, input_width )\n",
    "                 inp_shape,\n",
    "                 activation='tanh',\n",
    "                 # filter_shape = (filter_height, filter_width)\n",
    "                 filter_shape=(1, 1),\n",
    "                 lr=0.01,\n",
    "                 Co=1,\n",
    "                 seed=42): # This is the constructor of the class which initialises inp_shape, activation, filter_shape, lr, Co and seed where lr is the learning rate and Co is the number of output channels and Ci is the number of input channels\n",
    "\n",
    "        inp = np.random.rand(*inp_shape) # random input\n",
    "        np.random.seed(seed) # for reproducability of code\n",
    "        # Check if filter is valid or NOT by comparing input and filter shape\n",
    "        assert (inp_shape[1] >= filter_shape[0] and inp_shape[2] >= filter_shape[1]), \\\n",
    "            \"Error : Input {} incompatible with filter {}\".format(\n",
    "                inp.shape, filter_shape) # check if the input is compatible with the filter\n",
    "\n",
    "        self.inp = np.random.rand(*inp_shape) # random input\n",
    "        self.inp_shape = inp_shape # input shape\n",
    "        # number of channels in input here denoted as inp\n",
    "\n",
    "        self.Ci = self.inp.shape[0] # number of input channels\n",
    "        self.Co = Co # number of output channels\n",
    "        self.filters_shape = (self.Co, self.Ci,  *filter_shape) # filter shape\n",
    "        self.out_shape = (self.Co, self.inp.shape[1] - filter_shape[0] + 1, self.inp.shape[2] - filter_shape[1] + 1) # output shape\n",
    "        self.flatten_shape = np.prod(self.out_shape) # flattened shape\n",
    "        self.lr = lr # learning rate\n",
    "\n",
    "        self.filters = np.random.rand(*self.filters_shape) # random filters\n",
    "        self.biases = np.random.rand(*self.out_shape) # random biases\n",
    "        self.out = np.random.rand(*self.out_shape) # random output\n",
    "        self.flatten_out = np.random.rand(1, self.flatten_shape) # random flattened output\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.activation_layer = tanhActivation(self.out) # create an instance of the tanhActivation class\n",
    "\n",
    "    \"\"\"\n",
    "    The forward pass works as follows:\n",
    "    The input X is passed to the convolutional layer which applies the forward pass to the input X to get the next output Z\n",
    "    and then this output Z is passed to the activation layer which applies the activation function to this output Z to get the next output Z\n",
    "    and then this output Z is the final output of the convolutional layer\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, ): # forward pass of the convolutional layer\n",
    "        self.out = np.copy(self.biases) # output after convolution\n",
    "        for i in range(self.Co):  # for each output channel\n",
    "            for j in range(self.Ci): # for each input channel\n",
    "                self.out[i] += self.convolve(self.inp[j], self.filters[i, j])  # convolution operation\n",
    "\n",
    "        self.flatten() # flatten the output\n",
    "        self.activation_layer.Z = self.flatten_out # input to the activation layer\n",
    "        self.activation_layer.forward() # forward pass of the activation layer\n",
    "\n",
    "    \"\"\"\n",
    "    The backward pass works as follows:\n",
    "    The output Z is passed to the activation layer which applies the activation function to the output Z\n",
    "    to get the derivative of the output Z with respect to the input and then this derivative is passed to the convolutional layer\n",
    "    which gets the derivative of the previous output Z with respect to the input and the filter matrix and then this derivative is the final derivative of the output Z\n",
    "    \"\"\"\n",
    "\n",
    "    def backward(self, grad_nn): # backward pass of the convolutional layer\n",
    "\n",
    "        self.activation_layer.backward() # backward pass of the activation layer\n",
    "        loss_gradient = np.dot(self.activation_layer.daZ_dZ, grad_nn) # loss gradient\n",
    "        # reshape to (Co, H_out, W_out)\n",
    "        loss_gradient = np.reshape(loss_gradient, self.out_shape) # reshape the loss gradient\n",
    "\n",
    "        # dL/dKij for each filter  Kij    1<=i<=Ci , 1<=j<=Co\n",
    "        self.filters_gradient = np.zeros(self.filters_shape) # dL/dKij\n",
    "        self.input_gradient = np.zeros(self.inp_shape)  # dL/dXj\n",
    "        self.biases_gradient = loss_gradient  # dL/dBi  = dL/dYi\n",
    "        padded_loss_gradient = np.pad(loss_gradient, ((0, 0), (self.filters_shape[2]-1, self.filters_shape[2]-1), (self.filters_shape[3]-1, self.filters_shape[3]-1))) # padded loss gradient\n",
    "\n",
    "        for i in range(self.Co): # for each output channel\n",
    "            for j in range(self.Ci): # for each input channel\n",
    "                self.filters_gradient[i, j] = self.convolve(self.inp[j], loss_gradient[i])  # dL/dKij = convolution( Xj, dL/dYi)\n",
    "                rot180_Kij = np.rot90(np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))  # rotate the filter matrix by 180 degree\n",
    "                self.input_gradient[j] += self.convolve(padded_loss_gradient[i], rot180_Kij) # dL/dXj = convolution( dL/dYi, rot180(Kij))\n",
    "\n",
    "        self.filters -= self.lr*self.filters_gradient # update filters\n",
    "        self.biases -= self.lr*self.biases_gradient # update biases\n",
    "\n",
    "    # flattening output to 1 Dimension so it can be fed int neural network\n",
    "\n",
    "    def flatten(self, ):\n",
    "        self.flatten_out = self.out.reshape(1, -1) # flatten the output\n",
    "\n",
    "    # convolutional operation with stride=1, where stride is the number of pixels by which we slide the filter matrix over the input matrix\n",
    "    def convolve(self, x, y): # convolution operation\n",
    "        x_conv_y = np.zeros((x.shape[0] - y.shape[0] + 1, x.shape[1] - y.shape[1] + 1)) # output after convolution\n",
    "        for i in range(x.shape[0]-y.shape[0] + 1): # for each row\n",
    "            for j in range(x.shape[1] - y.shape[1] + 1): # for each column\n",
    "                tmp = x[i:i+y.shape[0], j:j+y.shape[1]] # temporary matrix\n",
    "                tmp = np.multiply(tmp, y) # element-wise multiplication of the input matrix and the filter matrix\n",
    "                x_conv_y[i, j] = np.sum(tmp) # sum of the elements of the temporary matrix\n",
    "        return x_conv_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN : # Implementation of Convolutional Neural Network\n",
    "    # In this class we are basically generating the input and output for the Convolutional Layer and Neural Network\n",
    "    \n",
    "    def __init__(self, \n",
    "                convolutional_layer,                 \n",
    "                nn,                                    \n",
    "                seed = 42): \n",
    "\n",
    "        self.nn = nn # feed forward neural network\n",
    "        self.convolutional_layer = convolutional_layer  # convolutional layer\n",
    "        self.X = np.random.rand(*self.convolutional_layer.inp_shape) # random input\n",
    "        self.Y = np.random.rand(*self.nn.out_shape) # random output\n",
    "    \n",
    "    \"\"\"\n",
    "    The forward pass works as follows:\n",
    "    The input X is passed to the convolutional layer which applies the forward pass to the input X to get the next output Z\n",
    "    and then this output Z is passed to the neural network which applies the forward pass to this output Z to get the next output Z\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self,): # forward pass of the convolutional neural network\n",
    "        self.convolutional_layer.inp = self.X # input to the convolutional layer\n",
    "        self.convolutional_layer.forward() # forward pass of the convolutional layer\n",
    "\n",
    "        self.nn.X = self.convolutional_layer.activation_layer.aZ # input to the neural network\n",
    "        self.nn.Y = self.Y  # true output\n",
    "        self.nn.forward()  # Forward Pass\n",
    "    \n",
    "    \"\"\"\n",
    "    The backward pass works as follows:\n",
    "    The predicted output Z is passed to the neural network which applies the backward pass to the predicted output Z to get the derivative\n",
    "    of the predicted output Z with respect to the true output and then this derivative is passed to the convolutional layer which applies the\n",
    "    backward pass to the derivative of the predicted output Z\n",
    "    \"\"\"\n",
    "\n",
    "    def backward(self,): # backward pass of the convolutional neural network\n",
    "        self.nn.backward() # Backward Pass\n",
    "        self.convolutional_layer.backward( self.nn.grad_nn )  # Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_CNN(X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cnn,\n",
    "            inp_shape,\n",
    "            out_shape,\n",
    "            n_iterations=1000,\n",
    "            task=\"classification\"): # This function is used to train the convolutional neural network model using stochastic gradient descent\n",
    "\n",
    "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100) # progress bar\n",
    "\n",
    "    for iteration, _ in enumerate(iterations): # train the model for each iteration\n",
    "        randomIndx = np.random.randint(len(X_train)) # randomly choose a sample subset of the data\n",
    "        X_sample = X_train[randomIndx, :].reshape(inp_shape) # input data\n",
    "        Y_sample = y_train[randomIndx, :].reshape(out_shape) # output data\n",
    "\n",
    "        cnn.X = X_sample # initialize the input data to the sample subset of the data\n",
    "        cnn.Y = Y_sample # initialize the output data to the sample subset of the data\n",
    "\n",
    "        cnn.forward()  # Forward Pass\n",
    "        cnn.backward()  # Backward Pass\n",
    "\n",
    "    # We'll run only forward pass for train and test data and check accuracy/error because we have already updated the weights and biases in the backward pass\n",
    "\n",
    "    if task == \"classification\": # check the accuracy for classification problems\n",
    "        X_train = X_train.reshape(-1, 8, 8) # reshape the input data\n",
    "        y_true = np.argmax(y_train, axis=1) # true output\n",
    "        acc = 0 # accuracy\n",
    "        for i in range(len(X_train)): # for each sample in the training data\n",
    "            cnn.X = X_train[i][np.newaxis, :, :] # input data\n",
    "            cnn.Y = y_train[i] # true output\n",
    "            cnn.forward() # forward pass of the convolutional neural network\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1) # predicted output\n",
    "            if (y_pred_i == y_true[i]): # check if the predicted output is equal to the true output\n",
    "                acc += 1 # increment the accuracy\n",
    "        \n",
    "        print(\"Classification Accuracy (Training Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" ) #str is used to convert the output to a string\n",
    "\n",
    "        X_test = X_test.reshape(-1, 8, 8) # reshape the input data\n",
    "        y_true = np.argmax(y_test, axis=1) # true output\n",
    "        acc = 0 # accuracy\n",
    "        for i in range(len(X_test)): # for each sample in the testing data\n",
    "            cnn.X = X_test[i][np.newaxis, :, :] # input data\n",
    "            cnn.Y = y_test[i] # true output\n",
    "            cnn.forward() # forward pass of the convolutional neural network\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1) # predicted output\n",
    "            if (y_pred_i == y_true[i]): # check if the predicted output is equal to the true output\n",
    "                acc += 1 # increment the accuracy\n",
    "        \n",
    "        print(\"Classification Accuracy (Testing Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" ) #str is used to convert the output to a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True) # load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...:   0%|▏                                              | 16/5000 [00:00<02:44, 30.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████████| 5000/5000 [02:25<00:00, 34.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ):1276/1437 = 88.79610299234517 %\n",
      "Classification Accuracy (Testing Data ):317/360 = 88.05555555555556 %\n"
     ]
    }
   ],
   "source": [
    "conv_inp_shape = (1,8,8)   # sklearn digit dataset has images of shape 1 x 8 x 8\n",
    "Co = 16  # 16 channel output \n",
    "conv_filter_shape = (3,3) # 3 x 3 filter\n",
    "conv_activation = 'tanh' # activation function for the convolutional layer\n",
    "convolutional_layer = ConvolutionalLayer(conv_inp_shape, \n",
    "                                        filter_shape = conv_filter_shape, \n",
    "                                        Co = Co,\n",
    "                                        activation = conv_activation,\n",
    "                                        lr = 0.01) # create an instance of the ConvolutionalLayer class\n",
    "nn_inp_shape = convolutional_layer.flatten_shape # input shape\n",
    "layers_sizes = [10] # number of neurons in the layer\n",
    "layers_activations = ['softmax'] # activation function for the layer\n",
    "\n",
    "nn_inp_shape, nn_out_shape, layers = createLayers(nn_inp_shape, layers_sizes, layers_activations) # create the layers of the neural network model\n",
    "loss_nn = 'cross_entropy' # loss function\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01) # create the neural network model\n",
    "\n",
    "cnn = CNN( convolutional_layer, nn) # create the convolutional neural network model\n",
    "out_shape =  (1, layers_sizes[-1])  # one_hot encoded ouptut \n",
    "\n",
    "SGD_CNN(X_train,y_train,X_test,y_test, cnn,conv_inp_shape, out_shape,n_iterations=5000) # train the convolutional neural network model using stochastic gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
